{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cell\n",
    "import pathlib\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if fodler trading_strat_data does not exist, create it\n",
    "pathlib.Path('trading_strat_data').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sanity check of the data, do we have indeed 30 stock at all time for the DJIA, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary written to summary_output.txt\n"
     ]
    }
   ],
   "source": [
    "# Path to the parent directory containing year folders\n",
    "data_dir = pathlib.Path(\"api-data-signal/\")\n",
    "\n",
    "year_to_companies = {}\n",
    "company_to_years = defaultdict(set)\n",
    "\n",
    "# Traverse each \"year\" folder\n",
    "for year_folder in data_dir.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        year = year_folder.name\n",
    "        companies = []\n",
    "        # Each subfolder inside 'year_folder' is treated as a ticker\n",
    "        for ticker_folder in year_folder.iterdir():\n",
    "            if ticker_folder.is_dir():\n",
    "                companies.append(ticker_folder.name)\n",
    "                company_to_years[ticker_folder.name].add(year)\n",
    "        year_to_companies[year] = companies\n",
    "\n",
    "# Build output lines\n",
    "output_lines = []\n",
    "output_lines.append(\"Yearly structure:\")\n",
    "for year, companies in sorted(year_to_companies.items()):\n",
    "    output_lines.append(f\"{year}: {len(companies)} companies\")\n",
    "    output_lines.append(f\"  {sorted(companies)}\")\n",
    "\n",
    "output_lines.append(\"\\nCompany appearances across years:\")\n",
    "for company, years in sorted(company_to_years.items()):\n",
    "    output_lines.append(f\"{company}: appears in {len(years)} year(s)\")\n",
    "    output_lines.append(f\"  {sorted(years)}\")\n",
    "\n",
    "# Write to a file in the \"trading_strat_data/\" folder\n",
    "with open(\"trading_strat_data/summary_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in output_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Summary written to summary_output.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create single dataframe containing the sentiment values of interest for each stock, each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Company  sentiment_score_positive  sentiment_score_negative  \\\n",
      "0  2010    AAPL                      66.0                     144.0   \n",
      "1  2010    AMGN                     106.0                     106.0   \n",
      "2  2010    AMZN                     110.0                      92.0   \n",
      "3  2010     AXP                       1.0                       0.0   \n",
      "4  2010      BA                     183.0                     379.0   \n",
      "\n",
      "   sentiment_score_polarity  sentiment_score_subjectivity  similarity_score  \\\n",
      "0                 -0.371429                      0.047436          0.962567   \n",
      "1                  0.000000                      0.045778          0.962118   \n",
      "2                  0.089109                      0.056933          0.000000   \n",
      "3                  0.999999                      0.041667          0.000000   \n",
      "4                 -0.348754                      0.073889          0.982955   \n",
      "\n",
      "   nlp_result  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "DataFrame saved to sentiment_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the folder containing the year folders\n",
    "data_dir = pathlib.Path(\"api-data-signal/\")\n",
    "\n",
    "# List to hold rows for the DataFrame\n",
    "rows = []\n",
    "\n",
    "# Iterate over each year folder\n",
    "for year_folder in data_dir.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        year = year_folder.name  # e.g., \"2010\", \"2011\", etc.\n",
    "        # Iterate over each company folder within the year folder\n",
    "        for company_folder in year_folder.iterdir():\n",
    "            if company_folder.is_dir():\n",
    "                company = company_folder.name  # e.g., \"AAPL\", \"AMZN\", etc.\n",
    "                info_file = company_folder / \"10_K_info.txt\"\n",
    "                if info_file.exists():\n",
    "                    try:\n",
    "                        # Read and parse the JSON content from the file\n",
    "                        with open(info_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                            content = f.read().strip()\n",
    "                            # If the file is pure JSON, we can load it directly\n",
    "                            info = json.loads(content)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {info_file}: {e}\")\n",
    "                        info = {}\n",
    "                    \n",
    "                    # Extract sentiment and other values\n",
    "                    sentiment = info.get(\"sentiment_score\", {})\n",
    "                    row = {\n",
    "                        \"Year\": year,\n",
    "                        \"Company\": company,\n",
    "                        \"sentiment_score_positive\": sentiment.get(\"Positive\"),\n",
    "                        \"sentiment_score_negative\": sentiment.get(\"Negative\"),\n",
    "                        \"sentiment_score_polarity\": sentiment.get(\"Polarity\"),\n",
    "                        \"sentiment_score_subjectivity\": sentiment.get(\"Subjectivity\"),\n",
    "                        \"similarity_score\": info.get(\"similarity_score\"),\n",
    "                        \"nlp_result\": info.get(\"nlp_result\")\n",
    "                    }\n",
    "                    rows.append(row)\n",
    "                else:\n",
    "                    print(f\"File not found: {info_file}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Optionally convert 'Year' to numeric and sort the DataFrame\n",
    "df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "df = df.sort_values(by=[\"Year\", \"Company\"]).reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"trading_strat_data/sentiment_analysis.csv\", index=False)\n",
    "print(\"DataFrame saved to sentiment_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Count  Percentage\n",
      "Year                              0    0.000000\n",
      "Company                           0    0.000000\n",
      "sentiment_score_positive         32    7.619048\n",
      "sentiment_score_negative         81   19.285714\n",
      "sentiment_score_polarity         37    8.809524\n",
      "sentiment_score_subjectivity     30    7.142857\n",
      "similarity_score                  4    0.952381\n",
      "nlp_result                      420  100.000000\n"
     ]
    }
   ],
   "source": [
    "# check the number of 0's in the dataframe df, within each column, and their percentage\n",
    "zero_counts = df.isin([0]).sum()\n",
    "zero_percentages = df.isin([0]).mean() * 100\n",
    "print(pd.concat([zero_counts, zero_percentages], axis=1, keys=['Count', 'Percentage']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve market data of ADJUSTED returns (bc of stock splits, etc), and taking for year i+1, then compute returns timeframe for 1)stocks 2)DJIA as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  30 of 30 completed\n",
      "/var/folders/cz/tndvsnfj5pbc0hg4zwfqf21c0000gn/T/ipykernel_21146/183208183.py:17: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  yearly_first = stocks_data.resample(\"Y\").first()\n",
      "/var/folders/cz/tndvsnfj5pbc0hg4zwfqf21c0000gn/T/ipykernel_21146/183208183.py:18: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  yearly_last = stocks_data.resample(\"Y\").last()\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yearly Stock Returns:\n",
      "Ticker          AAPL      AMGN      AMZN       AXP        BA       CAT  \\\n",
      "Date                                                                     \n",
      "2011-12-31  0.228874  0.167906 -0.060363  0.104228  0.131701 -0.019657   \n",
      "2012-12-31  0.305586  0.370443  0.401274  0.204688  0.039845 -0.020585   \n",
      "2013-12-31  0.047508  0.303361  0.549843  0.558365  0.806242 -0.008824   \n",
      "2014-12-31  0.426284  0.402192 -0.220167  0.051461 -0.026749  0.045588   \n",
      "2015-12-31 -0.020822  0.035653  1.190749 -0.241932  0.140914 -0.232671   \n",
      "\n",
      "Ticker           CRM      CSCO       CVX       DIS  ...      MSFT       NKE  \\\n",
      "Date                                                ...                       \n",
      "2011-12-31 -0.257247 -0.107641  0.193773  0.008189  ... -0.047545  0.135907   \n",
      "2012-12-31  0.661067  0.079512  0.012824  0.319618  ...  0.025976  0.081710   \n",
      "2013-12-31  0.289712  0.127668  0.168769  0.513566  ...  0.395438  0.537844   \n",
      "2014-12-31  0.081115  0.294883 -0.064119  0.250651  ...  0.284228  0.243875   \n",
      "2015-12-31  0.323430  0.006924 -0.164685  0.134599  ...  0.218785  0.329359   \n",
      "\n",
      "Ticker          NVDA        PG       SHW       TRV       UNH         V  \\\n",
      "Date                                                                     \n",
      "2011-12-31 -0.123894  0.063362  0.072218  0.090945  0.382829  0.451195   \n",
      "2012-12-31 -0.121147  0.050147  0.711657  0.252323  0.068942  0.483339   \n",
      "2013-12-31  0.286951  0.209735  0.184476  0.272438  0.402673  0.444355   \n",
      "2014-12-31  0.286810  0.166707  0.457184  0.212318  0.377920  0.195323   \n",
      "2015-12-31  0.664514 -0.093225 -0.001233  0.095199  0.186351  0.178697   \n",
      "\n",
      "Ticker            VZ       WMT  \n",
      "Date                            \n",
      "2011-12-31  0.160831  0.125302  \n",
      "2012-12-31  0.142872  0.158836  \n",
      "2013-12-31  0.159238  0.164401  \n",
      "2014-12-31 -0.002634  0.115590  \n",
      "2015-12-31  0.032227 -0.266537  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Yearly DJIA Returns:\n",
      "Ticker          ^DJI\n",
      "Date                \n",
      "2011-12-31  0.046853\n",
      "2012-12-31  0.057009\n",
      "2013-12-31  0.235907\n",
      "2014-12-31  0.084039\n",
      "2015-12-31 -0.022877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/cz/tndvsnfj5pbc0hg4zwfqf21c0000gn/T/ipykernel_21146/183208183.py:25: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  djia_yearly_first = djia_data.resample(\"Y\").first()\n",
      "/var/folders/cz/tndvsnfj5pbc0hg4zwfqf21c0000gn/T/ipykernel_21146/183208183.py:26: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  djia_yearly_last = djia_data.resample(\"Y\").last()\n"
     ]
    }
   ],
   "source": [
    "# Define tickers for stocks (example: 30 DJIA companies) and the DJIA ticker\n",
    "tickers = [\n",
    "    \"AAPL\", \"AMGN\", \"AMZN\", \"AXP\", \"BA\", \"CAT\", \"CRM\", \"CSCO\", \"CVX\",\n",
    "    \"DIS\", \"GS\", \"HD\", \"HON\", \"IBM\", \"JNJ\", \"JPM\", \"KO\", \"MCD\", \"MMM\",\n",
    "    \"MRK\", \"MSFT\", \"NKE\", \"NVDA\", \"PG\", \"SHW\", \"TRV\", \"UNH\", \"V\", \"VZ\", \"WMT\"\n",
    "]\n",
    "djia_ticker = \"^DJI\"\n",
    "\n",
    "# Define date range\n",
    "start_date = \"2011-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "# Download adjusted daily closing data for stocks (adjusted prices)\n",
    "stocks_data = yf.download(tickers, start=start_date, end=end_date, auto_adjust=True)[\"Close\"]\n",
    "\n",
    "# Resample to yearly frequency: Get first and last prices of each year\n",
    "yearly_first = stocks_data.resample(\"Y\").first()\n",
    "yearly_last = stocks_data.resample(\"Y\").last()\n",
    "\n",
    "# Calculate yearly returns as (last / first) - 1\n",
    "yearly_returns = (yearly_last / yearly_first) - 1\n",
    "\n",
    "# Download adjusted DJIA data and compute yearly returns similarly\n",
    "djia_data = yf.download(djia_ticker, start=start_date, end=end_date, auto_adjust=True)[\"Close\"]\n",
    "djia_yearly_first = djia_data.resample(\"Y\").first()\n",
    "djia_yearly_last = djia_data.resample(\"Y\").last()\n",
    "djia_yearly_returns = (djia_yearly_last / djia_yearly_first) - 1\n",
    "\n",
    "# Create DataFrames to display\n",
    "print(\"Yearly Stock Returns:\")\n",
    "print(yearly_returns.head())\n",
    "\n",
    "print(\"\\nYearly DJIA Returns:\")\n",
    "print(djia_yearly_returns.head())\n",
    "\n",
    "# Optionally, save the results to CSV files:\n",
    "yearly_returns.to_csv(\"trading_strat_data/yearly_stock_returns.csv\")\n",
    "djia_yearly_returns.to_csv(\"trading_strat_data/yearly_djia_returns.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the DJIA yearly returns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(djia_yearly_returns, marker=\"o\", color=\"b\", label=\"DJIA\")\n",
    "plt.title(\"Yearly DJIA Returns\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Returns\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"trading_strat_data/yearly_djia_returns.png\")\n",
    "plt.close()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Data Overview\n",
    "\n",
    "This project has generated three key datasets that serve as the foundation for further analysis:\n",
    "\n",
    "1. **Yearly Returns for Each DJIA Stock**\n",
    "   - **Description:**  \n",
    "     A DataFrame where each row represents a year and each column represents one of the 30 DJIA stocks.\n",
    "   - **Calculation:**  \n",
    "     Yearly return is calculated using the formula:  \n",
    "     \\[\n",
    "     \\text{Return} = \\left(\\frac{\\text{Last Adjusted Price}}{\\text{First Adjusted Price}}\\right) - 1\n",
    "     \\]\n",
    "   - **Purpose:**  \n",
    "     Provides an overview of the annual performance of each stock.\n",
    "\n",
    "2. **Yearly Returns for the DJIA Index**\n",
    "   - **Description:**  \n",
    "     A separate DataFrame showing the yearly return for the DJIA index as a whole (using the `^DJI` ticker).\n",
    "   - **Calculation:**  \n",
    "     Calculated using the same method as the individual stocks.\n",
    "   - **Purpose:**  \n",
    "     Offers a benchmark for the overall market performance.\n",
    "\n",
    "3. **Sentiment Analysis DataFrame**\n",
    "   - **Description:**  \n",
    "     A DataFrame that contains sentiment analysis metrics for each company (ticker) for each year.\n",
    "   - **Metrics Included:**  \n",
    "     - `sentiment_score_positive`: Number of positive sentiment counts.\n",
    "     - `sentiment_score_negative`: Number of negative sentiment counts.\n",
    "     - `sentiment_score_polarity`: Overall polarity of the sentiment.\n",
    "     - `sentiment_score_subjectivity`: How subjective the sentiment is.\n",
    "     - `similarity_score`: Similarity metric from NLP analysis.\n",
    "     - `nlp_result`: Additional NLP-derived result.\n",
    "   - **Purpose:**  \n",
    "     Enables analysis of textual sentiment in relation to each company’s performance and trends over the years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all dataset in logical way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Company  sentiment_score_positive  sentiment_score_negative  \\\n",
      "0  2011    AAPL                      66.0                     144.0   \n",
      "1  2011    AMGN                     106.0                     106.0   \n",
      "2  2011    AMZN                     110.0                      92.0   \n",
      "3  2011     AXP                       1.0                       0.0   \n",
      "4  2011      BA                     183.0                     379.0   \n",
      "\n",
      "   sentiment_score_polarity  sentiment_score_subjectivity  similarity_score  \\\n",
      "0                 -0.371429                      0.047436          0.962567   \n",
      "1                  0.000000                      0.045778          0.962118   \n",
      "2                  0.089109                      0.056933          0.000000   \n",
      "3                  0.999999                      0.041667          0.000000   \n",
      "4                 -0.348754                      0.073889          0.982955   \n",
      "\n",
      "   nlp_result Yearly_Return  DJIA_Return  \n",
      "0           0      0.228874     0.046853  \n",
      "1           0      0.167906     0.046853  \n",
      "2           0     -0.060363     0.046853  \n",
      "3           0      0.104228     0.046853  \n",
      "4           0      0.131701     0.046853  \n",
      "Merged data saved to trading_strat_data/merged_data.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Sentiment Analysis CSV ---\n",
    "df_sentiment = pd.read_csv(\"trading_strat_data/sentiment_analysis.csv\")\n",
    "# Convert Year to numeric and shift by 1 so that sentiment for year i becomes year i+1\n",
    "df_sentiment[\"Year\"] = pd.to_numeric(df_sentiment[\"Year\"], errors=\"coerce\") + 1\n",
    "\n",
    "# --- 2. Load and Process the Yearly Stock Returns CSV ---\n",
    "df_stocks = pd.read_csv(\"trading_strat_data/yearly_stock_returns.csv\", parse_dates=True)\n",
    "if \"Year\" not in df_stocks.columns:\n",
    "    if \"Date\" in df_stocks.columns:\n",
    "        df_stocks[\"Year\"] = pd.to_datetime(df_stocks[\"Date\"]).dt.year\n",
    "    else:\n",
    "        df_stocks = df_stocks.reset_index()\n",
    "        if \"index\" in df_stocks.columns:\n",
    "            df_stocks = df_stocks.rename(columns={\"index\": \"Date\"})\n",
    "        df_stocks[\"Year\"] = pd.to_datetime(df_stocks[\"Date\"]).dt.year\n",
    "df_stocks_long = pd.melt(df_stocks, id_vars=[\"Year\"], var_name=\"Company\", value_name=\"Yearly_Return\")\n",
    "\n",
    "# --- 3. Load and Process the Yearly DJIA Returns CSV ---\n",
    "df_djia = pd.read_csv(\"trading_strat_data/yearly_djia_returns.csv\", parse_dates=True)\n",
    "if \"Year\" not in df_djia.columns:\n",
    "    if \"Date\" in df_djia.columns:\n",
    "        df_djia[\"Year\"] = pd.to_datetime(df_djia[\"Date\"]).dt.year\n",
    "    else:\n",
    "        df_djia = df_djia.reset_index()\n",
    "        if \"index\" in df_djia.columns:\n",
    "            df_djia = df_djia.rename(columns={\"index\": \"Date\"})\n",
    "        df_djia[\"Year\"] = pd.to_datetime(df_djia[\"Date\"]).dt.year\n",
    "djia_return_col = [col for col in df_djia.columns if col not in [\"Year\", \"Date\"]][0]\n",
    "df_djia = df_djia.rename(columns={djia_return_col: \"DJIA_Return\"})\n",
    "\n",
    "# --- 4. Merge the DataFrames ---\n",
    "df_merge = pd.merge(df_sentiment, df_stocks_long, on=[\"Year\", \"Company\"], how=\"inner\")\n",
    "df_final = pd.merge(df_merge, df_djia[[\"Year\", \"DJIA_Return\"]], on=\"Year\", how=\"left\")\n",
    "\n",
    "# --- 5. Output ---\n",
    "print(df_final.head())\n",
    "df_final.to_csv(\"trading_strat_data/merged_data.csv\", index=False)\n",
    "print(\"Merged data saved to trading_strat_data/merged_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# # Load the merged data (merged_data.csv already has sentiment and Yearly_Return)\n",
    "# df = pd.read_csv(\"trading_strat_data/merged_data.csv\")\n",
    "\n",
    "# # Sort by Company and Year\n",
    "# df = df.sort_values(by=[\"Company\", \"Year\"])\n",
    "\n",
    "# # Create target: shift Yearly_Return by -1 so that sentiment for year i predicts return in year i+1\n",
    "# df[\"Target_Return\"] = df.groupby(\"Company\")[\"Yearly_Return\"].shift(-1)\n",
    "# df = df.dropna(subset=[\"Target_Return\"])  # Drop rows where target is missing\n",
    "\n",
    "# # Define features (sentiment metrics) and target variable\n",
    "# features = [\n",
    "#     \"sentiment_score_positive\", \n",
    "#     \"sentiment_score_negative\", \n",
    "#     \"sentiment_score_polarity\", \n",
    "#     \"sentiment_score_subjectivity\", \n",
    "#     \"similarity_score\"\n",
    "# ]\n",
    "# target = \"Target_Return\"\n",
    "\n",
    "# # Ensure Year is numeric\n",
    "# df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "\n",
    "# # Split data chronologically:\n",
    "# # Training: rows with Year ≤ 2018 (sentiment from i predicts return for i+1, where i is ≤2018)\n",
    "# # Testing: rows with Year > 2018\n",
    "# train = df[df[\"Year\"] <= 2018]\n",
    "# test = df[df[\"Year\"] > 2018]\n",
    "\n",
    "# X_train = train[features]\n",
    "# y_train = train[target]\n",
    "# X_test = test[features]\n",
    "# y_test = test[target]\n",
    "\n",
    "# # Train a linear regression model\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate model\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(\"Test MAE:\", mae)\n",
    "# print(\"Test R2 Score:\", r2)\n",
    "\n",
    "# # Display sample predictions with actual values\n",
    "# results = test.copy()\n",
    "# results[\"Predicted_Return\"] = y_pred\n",
    "# print(results[[\"Year\", \"Company\", \"Yearly_Return\", \"Target_Return\", \"Predicted_Return\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take all column from ´company´ to ´similarity_score´ as variables, regress targetting returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on test set: 0.1338316475881662\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the merged data (assume it's saved as 'merged_data.csv')\n",
    "df = pd.read_csv(\"trading_strat_data/merged_data.csv\")\n",
    "\n",
    "# Keep only the relevant columns (ignoring DJIA_Return)\n",
    "columns_to_keep = [\n",
    "    \"Year\", \"Company\", \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\",\n",
    "    \"Yearly_Return\"\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# One-hot encode the 'Company' column\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "company_encoded = encoder.fit_transform(df[['Company']])\n",
    "company_encoded_df = pd.DataFrame(company_encoded, columns=encoder.get_feature_names_out(['Company']))\n",
    "df = pd.concat([df.reset_index(drop=True), company_encoded_df.reset_index(drop=True)], axis=1)\n",
    "df.drop(columns=[\"Company\"], inplace=True)\n",
    "\n",
    "# Define the target and features\n",
    "target = \"Yearly_Return\"\n",
    "features = [\n",
    "    \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\"\n",
    "] + list(company_encoded_df.columns)\n",
    "\n",
    "# Sort data by Year to maintain time order\n",
    "df.sort_values(\"Year\", inplace=True)\n",
    "\n",
    "# Perform a time-based split: training on 2011-2020, testing on 2021-2024\n",
    "train_data = df[df[\"Year\"] <= 2020]\n",
    "test_data  = df[df[\"Year\"] > 2020]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_test  = test_data[features]\n",
    "y_test  = test_data[target]\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on test set:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take all column from ´sentiment_score_positive´ to ´similarity_score´ as variables, regress targetting returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on test set: 0.15925298615904512\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the merged data (assume it's saved as 'merged_data.csv')\n",
    "df = pd.read_csv(\"trading_strat_data/merged_data.csv\")\n",
    "\n",
    "# Keep only the relevant columns (ignoring DJIA_Return and Company)\n",
    "columns_to_keep = [\n",
    "    \"Year\", \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\",\n",
    "    \"Yearly_Return\"\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Define the target and features (Company is not used)\n",
    "target = \"Yearly_Return\"\n",
    "features = [\n",
    "    \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\"\n",
    "]\n",
    "\n",
    "# Sort data by Year to maintain time order\n",
    "df.sort_values(\"Year\", inplace=True)\n",
    "\n",
    "# Perform a time-based split: training on 2011-2020, testing on 2021-2024\n",
    "train_data = df[df[\"Year\"] <= 2020]\n",
    "test_data  = df[df[\"Year\"] > 2020]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_test  = test_data[features]\n",
    "y_test  = test_data[target]\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on test set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrebrun/miniconda3/envs/data_analysis_finance/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0963 - val_loss: 0.1059\n",
      "Epoch 2/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0807 - val_loss: 0.0885\n",
      "Epoch 3/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0951 - val_loss: 0.0750\n",
      "Epoch 4/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0807 - val_loss: 0.0781\n",
      "Epoch 5/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0581 - val_loss: 0.0702\n",
      "Epoch 6/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0804 - val_loss: 0.0728\n",
      "Epoch 7/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0638 - val_loss: 0.0745\n",
      "Epoch 8/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0775 - val_loss: 0.0703\n",
      "Epoch 9/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0408 - val_loss: 0.0707\n",
      "Epoch 10/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0530 - val_loss: 0.0701\n",
      "Epoch 11/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0625 - val_loss: 0.0712\n",
      "Epoch 12/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0513 - val_loss: 0.0676\n",
      "Epoch 13/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0831 - val_loss: 0.0743\n",
      "Epoch 14/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0673 - val_loss: 0.0706\n",
      "Epoch 15/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0470 - val_loss: 0.0683\n",
      "Epoch 16/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0552 - val_loss: 0.0714\n",
      "Epoch 17/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0621 - val_loss: 0.0687\n",
      "Epoch 18/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0666 - val_loss: 0.0723\n",
      "Epoch 19/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0544 - val_loss: 0.0725\n",
      "Epoch 20/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0884 - val_loss: 0.0711\n",
      "Epoch 21/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0544 - val_loss: 0.0673\n",
      "Epoch 22/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0415 - val_loss: 0.0721\n",
      "Epoch 23/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0679 - val_loss: 0.0725\n",
      "Epoch 24/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0950 - val_loss: 0.0687\n",
      "Epoch 25/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0375 - val_loss: 0.0703\n",
      "Epoch 26/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0568 - val_loss: 0.0723\n",
      "Epoch 27/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0469 - val_loss: 0.0695\n",
      "Epoch 28/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0592 - val_loss: 0.0706\n",
      "Epoch 29/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0660 - val_loss: 0.0708\n",
      "Epoch 30/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0441 - val_loss: 0.0720\n",
      "Epoch 31/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0480 - val_loss: 0.0715\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "Mean Squared Error on test set: 0.16885550487646672\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the merged data (assume it's saved as 'merged_data.csv')\n",
    "df = pd.read_csv(\"trading_strat_data/merged_data.csv\")\n",
    "\n",
    "# Keep only the relevant columns (ignoring DJIA_Return and Company)\n",
    "columns_to_keep = [\n",
    "    \"Year\", \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\",\n",
    "    \"Yearly_Return\"\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Sort data by Year to maintain time order\n",
    "df.sort_values(\"Year\", inplace=True)\n",
    "\n",
    "# Define the target and features\n",
    "target = \"Yearly_Return\"\n",
    "features = [\n",
    "    \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\"\n",
    "]\n",
    "\n",
    "# Perform a time-based split: training on 2011-2020, testing on 2021-2024\n",
    "train_data = df[df[\"Year\"] <= 2020]\n",
    "test_data  = df[df[\"Year\"] > 2020]\n",
    "\n",
    "X_train = train_data[features].values\n",
    "y_train = train_data[target].values\n",
    "X_test  = test_data[features].values\n",
    "y_test  = test_data[target].values\n",
    "\n",
    "# Standardize features for better neural network training\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer for regression (no activation)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Setup early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=100, \n",
    "    batch_size=8, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on test set:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrebrun/miniconda3/envs/data_analysis_finance/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.4495 - val_loss: 0.2570 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8129 - val_loss: 0.2612 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2362 - val_loss: 0.2524 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8623 - val_loss: 0.2424 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7739 - val_loss: 0.2293 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5757 - val_loss: 0.2381 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5350 - val_loss: 0.2395 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3822 - val_loss: 0.2393 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4262 - val_loss: 0.2357 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3489 - val_loss: 0.2317 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3439 - val_loss: 0.2307 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3245 - val_loss: 0.2341 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3202 - val_loss: 0.2358 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3331 - val_loss: 0.2333 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2967 - val_loss: 0.2213 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3049 - val_loss: 0.2219 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2898 - val_loss: 0.2311 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2279 - val_loss: 0.2229 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2531 - val_loss: 0.2159 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2025 - val_loss: 0.2125 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2574 - val_loss: 0.2084 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2373 - val_loss: 0.2013 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1917 - val_loss: 0.1991 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2045 - val_loss: 0.1919 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2136 - val_loss: 0.1849 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1918 - val_loss: 0.1778 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2307 - val_loss: 0.1736 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1884 - val_loss: 0.1705 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1990 - val_loss: 0.1671 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2360 - val_loss: 0.1621 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3088 - val_loss: 0.1614 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1672 - val_loss: 0.1600 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1822 - val_loss: 0.1554 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1880 - val_loss: 0.1521 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2254 - val_loss: 0.1507 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1854 - val_loss: 0.1500 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1710 - val_loss: 0.1497 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1628 - val_loss: 0.1463 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1671 - val_loss: 0.1468 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1569 - val_loss: 0.1415 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1374 - val_loss: 0.1392 - learning_rate: 5.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2075 - val_loss: 0.1330 - learning_rate: 5.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1701 - val_loss: 0.1330 - learning_rate: 5.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1509 - val_loss: 0.1329 - learning_rate: 5.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1550 - val_loss: 0.1325 - learning_rate: 5.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1817 - val_loss: 0.1315 - learning_rate: 5.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1673 - val_loss: 0.1311 - learning_rate: 5.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1361 - val_loss: 0.1294 - learning_rate: 5.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1664 - val_loss: 0.1272 - learning_rate: 5.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1332 - val_loss: 0.1274 - learning_rate: 5.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1577 - val_loss: 0.1268 - learning_rate: 5.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1468 - val_loss: 0.1249 - learning_rate: 5.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1310 - val_loss: 0.1258 - learning_rate: 5.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1416 - val_loss: 0.1237 - learning_rate: 5.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1377 - val_loss: 0.1227 - learning_rate: 5.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1205 - val_loss: 0.1220 - learning_rate: 5.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1566 - val_loss: 0.1214 - learning_rate: 5.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1223 - val_loss: 0.1207 - learning_rate: 5.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1464 - val_loss: 0.1205 - learning_rate: 5.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1286 - val_loss: 0.1190 - learning_rate: 5.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1432 - val_loss: 0.1175 - learning_rate: 5.0000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1382 - val_loss: 0.1166 - learning_rate: 5.0000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1249 - val_loss: 0.1158 - learning_rate: 5.0000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1334 - val_loss: 0.1159 - learning_rate: 5.0000e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1171 - val_loss: 0.1147 - learning_rate: 5.0000e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1469 - val_loss: 0.1151 - learning_rate: 5.0000e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1674 - val_loss: 0.1148 - learning_rate: 5.0000e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1180 - val_loss: 0.1134 - learning_rate: 5.0000e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1192 - val_loss: 0.1124 - learning_rate: 5.0000e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1100 - val_loss: 0.1118 - learning_rate: 5.0000e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1099 - val_loss: 0.1124 - learning_rate: 5.0000e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1111 - val_loss: 0.1119 - learning_rate: 5.0000e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1331 - val_loss: 0.1106 - learning_rate: 5.0000e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1323 - val_loss: 0.1095 - learning_rate: 5.0000e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1286 - val_loss: 0.1085 - learning_rate: 5.0000e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1363 - val_loss: 0.1085 - learning_rate: 5.0000e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1077 - val_loss: 0.1090 - learning_rate: 5.0000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1305 - val_loss: 0.1086 - learning_rate: 5.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1202 - val_loss: 0.1081 - learning_rate: 5.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1164 - val_loss: 0.1080 - learning_rate: 5.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1215 - val_loss: 0.1073 - learning_rate: 5.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1234 - val_loss: 0.1061 - learning_rate: 5.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1275 - val_loss: 0.1052 - learning_rate: 5.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1353 - val_loss: 0.1042 - learning_rate: 5.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0997 - val_loss: 0.1036 - learning_rate: 5.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1200 - val_loss: 0.1028 - learning_rate: 5.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1216 - val_loss: 0.1027 - learning_rate: 5.0000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1093 - val_loss: 0.1034 - learning_rate: 5.0000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1121 - val_loss: 0.1034 - learning_rate: 5.0000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1306 - val_loss: 0.1038 - learning_rate: 5.0000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1184 - val_loss: 0.1032 - learning_rate: 5.0000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1192 - val_loss: 0.1034 - learning_rate: 2.5000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1534 - val_loss: 0.1035 - learning_rate: 2.5000e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1282 - val_loss: 0.1038 - learning_rate: 2.5000e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1316 - val_loss: 0.1033 - learning_rate: 2.5000e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1068 - val_loss: 0.1028 - learning_rate: 2.5000e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0948 - val_loss: 0.1032 - learning_rate: 1.2500e-04\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Mean Squared Error on test set: 0.15122592301340848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_csv(\"trading_strat_data/merged_data.csv\")\n",
    "columns_to_keep = [\n",
    "    \"Year\", \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\",\n",
    "    \"Yearly_Return\"\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "df.sort_values(\"Year\", inplace=True)\n",
    "\n",
    "target = \"Yearly_Return\"\n",
    "features = [\n",
    "    \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\"\n",
    "]\n",
    "\n",
    "train_data = df[df[\"Year\"] <= 2020]\n",
    "test_data  = df[df[\"Year\"] > 2020]\n",
    "\n",
    "X_train = train_data[features].values\n",
    "y_train = train_data[target].values\n",
    "X_test  = test_data[features].values\n",
    "y_test  = test_data[target].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build an enhanced neural network model\n",
    "model = Sequential()\n",
    "# First layer with L2 regularization, BatchNormalization, and Dropout\n",
    "model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second layer\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third layer (optional additional layer)\n",
    "model.add(Dense(8, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Additional callback to reduce learning rate when a plateau is reached\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=100, \n",
    "    batch_size=8, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on test set:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on test set: 0.15021198841130426\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the merged data (assume it's saved as 'trading_strat_data/merged_data.csv')\n",
    "df = pd.read_csv(\"trading_strat_data/merged_data.csv\")\n",
    "\n",
    "# Keep only the relevant columns (ignoring DJIA_Return and Company)\n",
    "columns_to_keep = [\n",
    "    \"Year\", \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\",\n",
    "    \"Yearly_Return\"\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Define the target and features (Company is not used)\n",
    "target = \"Yearly_Return\"\n",
    "features = [\n",
    "    \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\"\n",
    "]\n",
    "\n",
    "# Sort data by Year to maintain time order\n",
    "df.sort_values(\"Year\", inplace=True)\n",
    "\n",
    "# Perform a time-based split: training on 2011-2020, testing on 2021-2024\n",
    "train_data = df[df[\"Year\"] <= 2020]\n",
    "test_data  = df[df[\"Year\"] > 2020]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_test  = test_data[features]\n",
    "y_test  = test_data[target]\n",
    "\n",
    "# Initialize and train the XGBoost regressor\n",
    "xgb_model = XGBRegressor(\n",
    "    objective='reg:squarederror',  # Using squared error for regression\n",
    "    n_estimators=100,               # Number of trees\n",
    "    learning_rate=0.1,              # Learning rate\n",
    "    max_depth=3,                    # Maximum depth of trees\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on test set:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## back to reg (ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on test set: 0.15234047547600227\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the merged data (assume it's saved as 'trading_strat_data/merged_data.csv')\n",
    "df = pd.read_csv(\"trading_strat_data/merged_data.csv\")\n",
    "\n",
    "# Keep only the relevant columns (ignoring DJIA_Return and Company)\n",
    "columns_to_keep = [\n",
    "    \"Year\", \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\",\n",
    "    \"Yearly_Return\"\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Define the target and features (Company is not used)\n",
    "target = \"Yearly_Return\"\n",
    "features = [\n",
    "    \"sentiment_score_positive\", \"sentiment_score_negative\",\n",
    "    \"sentiment_score_polarity\", \"sentiment_score_subjectivity\", \"similarity_score\"\n",
    "]\n",
    "\n",
    "# Sort data by Year to maintain time order\n",
    "df.sort_values(\"Year\", inplace=True)\n",
    "\n",
    "# Perform a time-based split: training on 2011-2020, testing on 2021-2024\n",
    "train_data = df[df[\"Year\"] <= 2020]\n",
    "test_data  = df[df[\"Year\"] > 2020]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_test  = test_data[features]\n",
    "y_test  = test_data[target]\n",
    "\n",
    "# Initialize and train the Ridge regression model\n",
    "# Adjust alpha as necessary; a common starting point is alpha=1.0.\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on test set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) \n",
    "\n",
    "-try to pool all together/ not pool and keep company per company\n",
    "(training assuming that all synthax of files \"10_K_info.txt\" are in similar style: hence we don't trian company per company?)\n",
    "\n",
    "-try algo reg, NeuralNet, etc?\n",
    "\n",
    "-which inputs\n",
    "\n",
    "## II)\n",
    "Trading strat and backtest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
