{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrebrun/miniconda3/envs/data_analysis_finance/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import codecs\n",
    "import json\n",
    "import pysentiment2 as ps\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_signal(text):\n",
    "    \"\"\"Computes sentiment scores based on Loughran and McDonald dictionary.\"\"\"\n",
    "    lm = ps.LM()\n",
    "    tokens = lm.tokenize(text)\n",
    "    score = lm.get_score(tokens)\n",
    "    score = {key: (int(value) if isinstance(value, np.integer) else float(value))\n",
    "             for key, value in score.items()}\n",
    "    return score\n",
    "\n",
    "\n",
    "def language_similarity(prev_text, curr_text):\n",
    "    \"\"\"Computes cosine similarity between two Item 7 filings.\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([prev_text, curr_text])\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return similarity[0][0]\n",
    "\n",
    "\n",
    "def advanced_nlp_analysis(text):\n",
    "    \"\"\"Uses BERT-based model for sentiment and topic classification.\"\"\"\n",
    "    bert_sentiment = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
    "    chunk_size = 512\n",
    "    chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    scores = []\n",
    "    for chunk in chunks:\n",
    "        result = bert_sentiment(chunk)[0]\n",
    "        label = result['label']\n",
    "        match label:\n",
    "            case 'positive':\n",
    "                score = 1\n",
    "            case 'negative':\n",
    "                score = -1\n",
    "            case 'neutral':\n",
    "                score = 0\n",
    "        scores.append(score * result['score'])\n",
    "    \n",
    "    return sum(scores) / max(len(scores), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(year, ticker):\n",
    "    output_dir = pathlib.Path(f'api-data/{year}/{ticker}')\n",
    "    with codecs.open(f'{output_dir}/10_K_info.txt', 'r', 'utf-8', errors='ignore') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_tickers(tickers, years):\n",
    "    \"\"\"Processes all tickers and years, applying signals to Item 7 text.\"\"\"\n",
    "    for i, year in tqdm(enumerate(years)):\n",
    "        for ticker in tickers:\n",
    "            curr_data = read_file(year, ticker)\n",
    "            if not curr_data:\n",
    "                continue\n",
    "            curr_text = curr_data.get(\"item_7_text\", \"\")\n",
    "            prev_text = None\n",
    "\n",
    "            try:\n",
    "                prev_data = read_file(int(years[i])-1, ticker)\n",
    "                prev_text = prev_data.get(\"item_7_text\", \"\")\n",
    "            except:\n",
    "                prev_text = \"\"\n",
    "            \n",
    "            curr_data[\"sentiment_score\"] = sentiment_analysis_signal(curr_text)\n",
    "            try:\n",
    "                curr_data[\"similarity_score\"] = language_similarity(prev_text, curr_text)\n",
    "            except:\n",
    "                curr_data[\"similarity_score\"] = 0\n",
    "            try:\n",
    "                curr_data[\"nlp_result\"] = advanced_nlp_analysis(curr_text)\n",
    "            except:\n",
    "                curr_data[\"nlp_result\"] = 0\n",
    "            \n",
    "            output_dir = pathlib.Path(f'api-data-signal/{year}/{ticker}')\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            with codecs.open(f'{output_dir}/10_K_info.txt', 'w', 'utf-8', errors='ignore') as f:\n",
    "                json.dump(curr_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [02:52, 12.30s/it]\n"
     ]
    }
   ],
   "source": [
    "tickers = [\n",
    "    \"AMZN\", \"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CSCO\", \"CVX\", \n",
    "    \"GS\", \"HD\", \"HON\", \"IBM\", \"JNJ\", \"KO\", \"JPM\", \"MCD\", \"MMM\", \n",
    "    \"MRK\", \"MSFT\", \"NKE\", \"PG\", \"SHW\", \"TRV\", \"UNH\", \"CRM\", \"NVDA\", \n",
    "    \"VZ\", \"V\", \"WMT\", \"DIS\"\n",
    "]\n",
    "years = ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
    "process_tickers(tickers, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2010': {'AXP': {'sentiment': {'Positive': 1, 'Negative': 0, 'Polarity': 0.9999990000010001, 'Subjectivity': 0.041666664930555625}, 'language_similarity': 0.0, 'nlp_analysis': 0.0}, 'MSFT': {'sentiment': {'Positive': 99, 'Negative': 120, 'Polarity': -0.09589041052104835, 'Subjectivity': 0.03631238599961658}, 'language_similarity': 0.9703686277651167, 'nlp_analysis': 0.05044493079185486}}, '2011': {'AXP': {'sentiment': {'Positive': 1, 'Negative': 0, 'Polarity': 0.9999990000010001, 'Subjectivity': 0.041666664930555625}, 'language_similarity': 0.8939305715734549, 'nlp_analysis': 0.0}, 'MSFT': {'sentiment': {'Positive': 96, 'Negative': 129, 'Polarity': -0.14666666601481482, 'Subjectivity': 0.03749999999375}, 'language_similarity': 0.9949837013659897, 'nlp_analysis': 0.05371086600887982}}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'api-data/2009/AXP/10_K_info.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2009\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAXP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(year, ticker)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_file\u001b[39m(year, ticker):\n\u001b[1;32m      2\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi-data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/10_K_info.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/anaconda3/envs/bikes-count/lib/python3.10/codecs.py:906\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    903\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# Force opening of the file in binary mode\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     mode \u001b[38;5;241m=\u001b[39m mode \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 906\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'api-data/2009/AXP/10_K_info.txt'"
     ]
    }
   ],
   "source": [
    "read_file(\"2009\", \"AXP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
